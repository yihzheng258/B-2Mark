{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabularMark\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset='covertype'\n",
    "\n",
    "original_file = '../dataset/covtype_with_key.subset.data'\n",
    "origin = pd.read_csv(original_file)\n",
    "\n",
    "n = int(len(origin)/8) \n",
    "gamma = 1/2 \n",
    "\n",
    "seeds = list(range(10000,10128))\n",
    "\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    divide_seeds = np.random.randint(0, 2**32 - 1, size=n)\n",
    "    indices = np.random.choice(len(origin), size=n, replace=False)\n",
    "\n",
    "    #添加水印\n",
    "    cover_types = origin['Cover_Type'].unique()\n",
    "\n",
    "    if len(indices) != len(divide_seeds):\n",
    "        raise ValueError(\"索引文件和种子文件的长度不一致\")\n",
    "    cover_types.sort()\n",
    "\n",
    "    for idx, divide_seed in zip(indices, divide_seeds):\n",
    "        np.random.seed(divide_seed)\n",
    "        candidate_set = cover_types\n",
    "\n",
    "        shuffled_cover_types = list(cover_types)\n",
    "        np.random.shuffle(shuffled_cover_types)\n",
    "        half_size = len(shuffled_cover_types) // 2\n",
    "\n",
    "        green_domain = shuffled_cover_types[:half_size]\n",
    "        red_domain = shuffled_cover_types[half_size:]\n",
    "\n",
    "        perturb_value = np.random.choice(green_domain)\n",
    "\n",
    "        origin.loc[idx, 'Cover_Type'] = perturb_value\n",
    "        \n",
    "    results = {\n",
    "        'watermarked_data': origin,\n",
    "        'divide_seeds': divide_seeds,\n",
    "        'indices': indices\n",
    "    }\n",
    "\n",
    "    np.save(f\"different_version_datasets/tabularmark/{dataset}-{seed}.npy\", results)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.83300132670377\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "seed = 10000\n",
    "dataset='covertype'\n",
    "\n",
    "\n",
    "original_file = '../dataset/covtype_with_key.subset.data'\n",
    "origin = pd.read_csv(original_file)\n",
    "\n",
    "n = int(len(origin)/8) \n",
    "gamma = 1/2 \n",
    "\n",
    "for i in range(128):\n",
    "    loaded_results = np.load(f\"different_version_datasets/tabularmark/{dataset}-{seed}.npy\", allow_pickle=True).item()\n",
    "    watermarked_data = loaded_results['watermarked_data']\n",
    "    divide_seeds = loaded_results['divide_seeds']\n",
    "    indices = loaded_results['indices']\n",
    "\n",
    "    cover_types = watermarked_data['Cover_Type'].unique()\n",
    "    cover_types.sort()  \n",
    "\n",
    "    green_cell = 0\n",
    "    for idx, divide_seed in zip(indices, divide_seeds):\n",
    "        np.random.seed(divide_seed)\n",
    "        candidate_set = cover_types\n",
    "        \n",
    "        shuffled_cover_types = list(cover_types)\n",
    "        \n",
    "        np.random.shuffle(shuffled_cover_types)\n",
    "\n",
    "        half_size = len(shuffled_cover_types) // 2\n",
    "\n",
    "        green_domain = shuffled_cover_types[:half_size]\n",
    "        red_domain = shuffled_cover_types[half_size:]\n",
    "\n",
    "        if watermarked_data.loc[idx, 'Cover_Type'] in green_domain:\n",
    "            green_cell += 1\n",
    "        \n",
    "    z_score = (green_cell - n/2) / math.sqrt(n/4)\n",
    "\n",
    "print(z_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "seed = 10000\n",
    "dataset='covertype'\n",
    "\n",
    "\n",
    "original_file = '../dataset/covtype_with_key.subset.data'\n",
    "origin = pd.read_csv(original_file)\n",
    "\n",
    "n = int(len(origin)/8) \n",
    "gamma = 1/2 \n",
    "\n",
    "primary_key_cols = ['Elevation', 'Aspect'] \n",
    "def binary_search(arr, key):\n",
    "    left, right = 0, len(arr) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if arr[mid] == key:\n",
    "            return mid\n",
    "        elif arr[mid] < key:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1\n",
    "    \n",
    "def match_tuples(origin_data, watermarked_data, indices):\n",
    "    # 初始化匹配索引集合\n",
    "    match_indices = []\n",
    "    watermarked_keys = [tuple(row) for row in watermarked_data[primary_key_cols].values]\n",
    "    for idx in indices:\n",
    "        key_do = tuple(origin_data.loc[idx, primary_key_cols])\n",
    "        match_idx = binary_search(watermarked_keys, key_do)\n",
    "        if match_idx != -1:\n",
    "            match_indices.append(watermarked_data.index[match_idx])\n",
    "        else:\n",
    "            match_indices.append(-1)\n",
    "        \n",
    "    return match_indices\n",
    "\n",
    "for seed in range(10000, 10128):\n",
    "    loaded_results = np.load(f\"different_version_datasets/tabularmark/{dataset}-{seed}.npy\", allow_pickle=True).item()\n",
    "    watermarked_data = loaded_results['watermarked_data']\n",
    "    watermarked_data = watermarked_data.sort_values(by=primary_key_cols).reset_index(drop=True)\n",
    "\n",
    "    divide_seeds = loaded_results['divide_seeds']\n",
    "    indices = loaded_results['indices']\n",
    "\n",
    "    cover_types = watermarked_data['Cover_Type'].unique()\n",
    "    cover_types.sort()  \n",
    "    green_cell = 0\n",
    "\n",
    "    match_indices = match_tuples(origin, watermarked_data, indices)\n",
    "    # print(match_indices)\n",
    "\n",
    "    for idx, divide_seed in zip(match_indices, divide_seeds):\n",
    "        np.random.seed(divide_seed)\n",
    "        candidate_set = cover_types\n",
    "\n",
    "        shuffled_cover_types = list(cover_types)\n",
    "        np.random.shuffle(shuffled_cover_types)\n",
    "\n",
    "        half_size = len(shuffled_cover_types) // 2\n",
    "\n",
    "        green_domain = shuffled_cover_types[:half_size]\n",
    "        red_domain = shuffled_cover_types[half_size:]\n",
    "\n",
    "        if watermarked_data.loc[idx, 'Cover_Type'] in green_domain:\n",
    "            green_cell += 1\n",
    "        \n",
    "    z_score = (green_cell - n/2) / math.sqrt(n/4)\n",
    "\n",
    "    # print(f\"z_score: {z_score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 16 files...\n",
      "Time taken for 16 files: 6.77 seconds\n",
      "\n",
      "Processing 32 files...\n",
      "Time taken for 32 files: 13.80 seconds\n",
      "\n",
      "Processing 48 files...\n",
      "Time taken for 48 files: 17.18 seconds\n",
      "\n",
      "Processing 64 files...\n",
      "Time taken for 64 files: 16.34 seconds\n",
      "\n",
      "Processing 80 files...\n",
      "Time taken for 80 files: 24.57 seconds\n",
      "\n",
      "Processing 96 files...\n",
      "Time taken for 96 files: 37.34 seconds\n",
      "\n",
      "Processing 112 files...\n",
      "Time taken for 112 files: 49.05 seconds\n",
      "\n",
      "Processing 128 files...\n",
      "Time taken for 128 files: 44.03 seconds\n",
      "\n",
      "Timing results:\n",
      "16 files: 6.77 seconds\n",
      "32 files: 13.80 seconds\n",
      "48 files: 17.18 seconds\n",
      "64 files: 16.34 seconds\n",
      "80 files: 24.57 seconds\n",
      "96 files: 37.34 seconds\n",
      "112 files: 49.05 seconds\n",
      "128 files: 44.03 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "seed_start = 10000\n",
    "seed_end = 10000 + 128  # 总共 128 份文件\n",
    "dataset = 'covertype'\n",
    "threshold = 40\n",
    "\n",
    "original_file = '../dataset/covtype_with_key.subset.data'\n",
    "origin = pd.read_csv(original_file)\n",
    "\n",
    "n = int(len(origin) / 8)\n",
    "gamma = 1 / 2\n",
    "\n",
    "primary_key_cols = ['Elevation', 'Aspect']\n",
    "\n",
    "# 二分查找函数\n",
    "def binary_search(arr, key):\n",
    "    left, right = 0, len(arr) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if arr[mid] == key:\n",
    "            return mid\n",
    "        elif arr[mid] < key:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1\n",
    "\n",
    "# 匹配元组函数\n",
    "def match_tuples(origin_data, watermarked_data, indices):\n",
    "    match_indices = []\n",
    "    watermarked_keys = [tuple(row) for row in watermarked_data[primary_key_cols].values]\n",
    "    for idx in indices:\n",
    "        key_do = tuple(origin_data.loc[idx, primary_key_cols])\n",
    "        match_idx = binary_search(watermarked_keys, key_do)\n",
    "        if match_idx != -1:\n",
    "            match_indices.append(watermarked_data.index[match_idx])\n",
    "        else:\n",
    "            match_indices.append(-1)\n",
    "    return match_indices\n",
    "\n",
    "# 测试不同数量文件所需时间\n",
    "file_counts = [16, 32, 48, 64, 80, 96, 112, 128]\n",
    "timing_results = {}\n",
    "\n",
    "\n",
    "for file_count in file_counts:\n",
    "    print(f\"Processing {file_count} files...\")\n",
    "    \n",
    "    random_files = np.random.choice(file_count, 10, replace=False)\n",
    "    \n",
    "    timing_results[file_count] = 0\n",
    "    for file in random_files:\n",
    "        start_time = time.time()  # 开始计时\n",
    "        watermarked_data = np.load(f\"different_version_datasets/tabularmark/{dataset}-{file+10000}.npy\", allow_pickle=True).item()\n",
    "        watermarked_data = watermarked_data['watermarked_data']\n",
    "        watermarked_data = watermarked_data.sort_values(by=primary_key_cols).reset_index(drop=True)\n",
    "    \n",
    "        for seed in range(seed_start, seed_start + file_count):\n",
    "            loaded_results = np.load(f\"different_version_datasets/tabularmark/{dataset}-{seed}.npy\", allow_pickle=True).item()\n",
    "\n",
    "            divide_seeds = loaded_results['divide_seeds']\n",
    "            indices = loaded_results['indices']\n",
    "            \n",
    "            cover_types = watermarked_data['Cover_Type'].unique()\n",
    "            cover_types.sort()\n",
    "\n",
    "            match_indices = match_tuples(origin, watermarked_data, indices)\n",
    "            green_cell = 0\n",
    "            for idx, divide_seed in zip(match_indices, divide_seeds):\n",
    "                np.random.seed(divide_seed)\n",
    "                candidate_set = cover_types\n",
    "\n",
    "                shuffled_cover_types = list(cover_types)\n",
    "                np.random.shuffle(shuffled_cover_types)\n",
    "\n",
    "                half_size = len(shuffled_cover_types) // 2\n",
    "\n",
    "                green_domain = shuffled_cover_types[:half_size]\n",
    "                red_domain = shuffled_cover_types[half_size:]\n",
    "\n",
    "                if idx != -1 and watermarked_data.loc[idx, 'Cover_Type'] in green_domain:\n",
    "                    green_cell += 1\n",
    "\n",
    "            z_score = (green_cell - n / 2) / math.sqrt(n / 4)\n",
    "            if(z_score > threshold):\n",
    "                # print(file)\n",
    "                # print(seed)\n",
    "                # print(f\"z_score: {z_score}\")\n",
    "                break\n",
    "        end_time = time.time()  # 结束计时\n",
    "        # print(f\"Time taken for {seed - seed_start} files: {end_time - start_time:.2f} seconds\")\n",
    "        timing_results[file_count] += end_time - start_time\n",
    "    timing_results[file_count] = timing_results[file_count] / 10\n",
    "    print(f\"Time taken for {file_count} files: {timing_results[file_count]:.2f} seconds\\n\")\n",
    "\n",
    "# 打印总结果\n",
    "print(\"Timing results:\")\n",
    "for file_count, duration in timing_results.items():\n",
    "    print(f\"{file_count} files: {duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 16 files...\n",
      "Average time for 16 files: 9.32 seconds\n",
      "\n",
      "Processing 32 files...\n",
      "Average time for 32 files: 9.29 seconds\n",
      "\n",
      "Processing 48 files...\n",
      "Average time for 48 files: 9.27 seconds\n",
      "\n",
      "Processing 64 files...\n",
      "Average time for 64 files: 9.27 seconds\n",
      "\n",
      "Processing 80 files...\n",
      "Average time for 80 files: 9.28 seconds\n",
      "\n",
      "Processing 96 files...\n",
      "Average time for 96 files: 9.35 seconds\n",
      "\n",
      "Processing 112 files...\n",
      "Average time for 112 files: 9.87 seconds\n",
      "\n",
      "Processing 128 files...\n",
      "Average time for 128 files: 9.55 seconds\n",
      "\n",
      "Timing results:\n",
      "16 files: 9.32 seconds\n",
      "32 files: 9.29 seconds\n",
      "48 files: 9.27 seconds\n",
      "64 files: 9.27 seconds\n",
      "80 files: 9.28 seconds\n",
      "96 files: 9.35 seconds\n",
      "112 files: 9.87 seconds\n",
      "128 files: 9.55 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "g = 8\n",
    "seed = 10000\n",
    "secret_key_1 = \"123\"\n",
    "secret_key_2 = \"456\"\n",
    "threshold = 3\n",
    "watermark_information_length = 8\n",
    "\n",
    "dataset = 'covertype'\n",
    "\n",
    "original_file = '../dataset/covtype_with_key.subset.data'\n",
    "origin = pd.read_csv(original_file)\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "cover_types = origin['Cover_Type'].unique()\n",
    "cover_types.sort()\n",
    "\n",
    "# 计算哈希取模\n",
    "def hash_mod(key, mod_value, secret_key):\n",
    "    combined = f\"{secret_key}{key}\"\n",
    "    hash_value = int(hashlib.sha256(combined.encode()).hexdigest(), 16)\n",
    "    return hash_value % mod_value\n",
    "\n",
    "columns_of_interest = ['Elevation', 'Aspect']\n",
    "origin[columns_of_interest] = origin[columns_of_interest].fillna(0)\n",
    "\n",
    "# 提取前两位非零数字\n",
    "def first_two_digits(x):\n",
    "    if x == 0:\n",
    "        return \"00\"\n",
    "    digits = str(x).lstrip('0.').replace('.', '')\n",
    "    if len(digits) == 1:\n",
    "        return digits + \"0\"\n",
    "    return digits[:2]\n",
    "\n",
    "# 测试不同数量文件的检测时间\n",
    "file_counts = [16, 32, 48, 64, 80, 96, 112, 128]\n",
    "timing_results = {}\n",
    "\n",
    "data_path = \"different_version_datasets/original\"\n",
    "\n",
    "for file_count in file_counts:\n",
    "    print(f\"Processing {file_count} files...\")\n",
    "    total_time = 0  # 累计时间\n",
    "    selected_files = sorted(os.listdir(data_path))[:file_count]  # 按顺序取前 file_count 个文件\n",
    "\n",
    "    for file_name in selected_files:\n",
    "        if file_name.startswith(\"covertype-\"):\n",
    "            start_time = time.time()  # 开始计时\n",
    "\n",
    "            watermarked_data = np.load(f\"{data_path}/{file_name}\", allow_pickle=True).item() \n",
    "            watermarked_data = watermarked_data['watermarked_data']\n",
    "\n",
    "            detected_watermark_information = \"\"\n",
    "            watermarked_data[columns_of_interest] = watermarked_data[columns_of_interest].fillna(0)\n",
    "\n",
    "            green_cells = np.zeros(watermark_information_length)\n",
    "            n_cells = np.zeros(watermark_information_length)\n",
    "            z_scores = np.zeros(watermark_information_length)\n",
    "\n",
    "            # 遍历数据进行检测\n",
    "            for idx in range(len(watermarked_data)):\n",
    "                selected_data = watermarked_data.loc[idx, columns_of_interest]\n",
    "                first_two_digits_data = selected_data.apply(first_two_digits)\n",
    "                composite_numbers = ''.join(first_two_digits_data.values)\n",
    "\n",
    "                w_index = hash_mod(composite_numbers, watermark_information_length, secret_key_1)\n",
    "\n",
    "                if hash_mod(composite_numbers, g, secret_key_2) == 0:\n",
    "                    n_cells[w_index] += 1\n",
    "                    if watermarked_data.loc[idx, 'Cover_Type'] in green_domain:\n",
    "                        green_cells[w_index] += 1\n",
    "\n",
    "            # 计算 z_scores\n",
    "            for idx in range(watermark_information_length):\n",
    "                if n_cells[idx] != 0:\n",
    "                    z_scores[idx] = (green_cells[idx] - n_cells[idx] / 2) / math.sqrt(n_cells[idx] / 4)\n",
    "                else:\n",
    "                    z_scores[idx] = 0\n",
    "\n",
    "            # 更新检测水印信息\n",
    "            for idx in range(len(z_scores)):\n",
    "                if z_scores[idx] > threshold:\n",
    "                    detected_watermark_information += '1'\n",
    "                else:\n",
    "                    detected_watermark_information += '0'\n",
    "\n",
    "            end_time = time.time()  # 结束计时\n",
    "            total_time += (end_time - start_time)  # 累计时间\n",
    "\n",
    "    # 求平均时间\n",
    "    average_time = total_time / file_count\n",
    "    timing_results[file_count] = average_time\n",
    "    print(f\"Average time for {file_count} files: {average_time:.2f} seconds\\n\")\n",
    "\n",
    "# 打印总结果\n",
    "print(\"Timing results:\")\n",
    "for file_count, duration in timing_results.items():\n",
    "    print(f\"{file_count} files: {duration:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
